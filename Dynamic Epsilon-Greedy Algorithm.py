import numpy as np
import matplotlib.pyplot as plt

# Reward probabilities function
def get_probabilities(drift=0):
    probs = [
        np.random.normal(0, 5),
        np.random.normal(-0.5, 12),
        np.random.normal(2, 3.9),
        np.random.normal(-0.5, 7),
        np.random.normal(-1.2, 8),
        np.random.normal(-3, 7),
        np.random.normal(-10, 20),
        np.random.normal(-0.5, 1),
        np.random.normal(-1, 2),
        np.random.normal(1, 6),
        np.random.normal(0.7, 4),
        np.random.normal(-6, 11),
        np.random.normal(-7, 1),
        np.random.normal(-0.5, 2),
        np.random.normal(-6.5, 1),
        np.random.normal(-3, 6),
        np.random.normal(0, 8),
        np.random.normal(2, 3.9),
        np.random.normal(-9, 12),
        np.random.normal(-1, 6),
        np.random.normal(-4.5, 8)
    ]
    return np.array(probs) + drift

# Epsilon-Greedy Algorithm
def epsilon_greedy(steps, epsilon, bandits, drift_rate, sudden_shifts_step=3000):
    n_bandits = len(bandits)
    values = np.zeros(n_bandits)  # Estimate of reward values
    counts = np.zeros(n_bandits)  # Number of times each bandit is chosen
    rewards = np.zeros(steps)
    for step in range(steps):
        # Apply gradual drift
        bandits += drift_rate
        
        # Apply sudden shifts at step 3000
        if step == sudden_shifts_step:
            bandits[0] += 7
            bandits[2] += 3
            bandits[7] += 1
            bandits[18] += 2
        
        # Epsilon-Greedy decision
        if np.random.rand() < epsilon:
            action = np.random.randint(0, n_bandits)  # Explore
        else:
            action = np.argmax(values)  # Exploit

        # Special case for Bandit 7
        if action == 7:
            std_bandit_7 = np.std([bandits[action]])  # Standard deviation for Bandit 7
            reward = np.random.normal(bandits[action])
            if abs(reward - bandits[action]) > 3 * std_bandit_7:  # Check if > 3 std dev
                reward = 50
        # Get Reward
        else:
            reward = np.random.normal(bandits[action])
        
        # Update values
        counts[action] += 1
        values[action] += (reward - values[action]) / counts[action]
        rewards[step] = reward
    return rewards.cumsum()

# Thompson Sampling Algorithm
def thompson_sampling(steps, bandits, drift_rate, sudden_shifts_step=3000, restart=False):
    n_bandits = len(bandits)
    alpha = np.ones(n_bandits)  # Successes
    beta = np.ones(n_bandits)   # Failures
    rewards = np.zeros(steps)
    for step in range(steps):
        # Apply gradual drift
        bandits += drift_rate
        
        # Apply sudden shifts at step 3000
        if step == sudden_shifts_step:
            bandits[0] += 7
            bandits[2] += 3
            bandits[7] += 1
            bandits[18] += 2
            if restart:  # Restart alpha and beta at step 3000
                alpha = np.ones(n_bandits)
                beta = np.ones(n_bandits)

        # Sample from Beta distribution for each bandit
        samples = [np.random.beta(a, b) for a, b in zip(alpha, beta)]
        action = np.argmax(samples)
        
        # Get reward
        # Special case for Bandit 7
        if action == 7:
            std_bandit_7 = np.std([bandits[action]])  # Standard deviation for Bandit 7
            reward = np.random.normal(bandits[action])
            if abs(reward - bandits[action]) > 3 * std_bandit_7:  # Check if > 3 std dev
                reward = 50
        else:
            reward = np.random.normal(bandits[action])
        rewards[step] = reward
        
        # Update alpha and beta based on reward
        if reward > 0:
            alpha[action] += 1  # Success
        else:
            beta[action] += 1  # Failure
    return rewards.cumsum()

# Simulating the environment
steps = 10000
drift_rate = -0.001
bandits = get_probabilities()

# Run Epsilon-Greedy for different epsilon values
epsilons = [0.01, 0.05, 0.1, 0.4]
greedy_results = {}
for eps in epsilons:
    bandits_initial = get_probabilities()  # Reset bandits
    greedy_results[eps] = epsilon_greedy(steps, eps, bandits_initial, drift_rate)

# Run Thompson Sampling (with and without restart)
bandits_initial = get_probabilities()  # Reset bandits
thompson_result_no_restart = thompson_sampling(steps, bandits_initial, drift_rate)
bandits_initial = get_probabilities()  # Reset bandits
thompson_result_with_restart = thompson_sampling(steps, bandits_initial, drift_rate, restart=True)

# Plot the results
plt.figure(figsize=(10, 6))
for eps, res in greedy_results.items():
    plt.plot(res, label=f'Epsilon-Greedy (Îµ={eps})')
plt.plot(thompson_result_no_restart, label='Thompson Sampling (No Restart)', linestyle='--')
plt.plot(thompson_result_with_restart, label='Thompson Sampling (Restart)', linestyle=':')
plt.title('Cumulative Reward: Epsilon-Greedy vs Thompson Sampling in Dynamic Environment')
plt.xlabel('Steps')
plt.ylabel('Cumulative Reward')
plt.legend()
plt.show()
