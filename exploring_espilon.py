import numpy as np
import matplotlib.pyplot as plt

# Function to generate reward probabilities
def get_probabilities():
    probs = [
        np.random.normal(0, 5),
        np.random.normal(-0.5, 12),
        np.random.normal(2, 3.9),
        np.random.normal(-0.5, 7),
        np.random.normal(-1.2, 8),
        np.random.normal(-3, 7),
        np.random.normal(-10, 20),
        np.random.normal(-0.5, 1),
        np.random.normal(-1, 2),
        np.random.normal(1, 6),
        np.random.normal(0.7, 4),
        np.random.normal(-6, 11),
        np.random.normal(-7, 1),
        np.random.normal(-0.5, 2),
        np.random.normal(-6.5, 1),
        np.random.normal(-3, 6),
        np.random.normal(0, 8),
        np.random.normal(2, 3.9),
        np.random.normal(-9, 12),
        np.random.normal(-1, 6),
        np.random.normal(-4.5, 8)
    ]
    return probs

# Quenching Functions
def linear_quench(t, total_steps): return max(0, 1 - t / total_steps)
def asymptotic_quench(t, total_steps): return 1 / (1 + 0.01 * t)
def heavy_asymptotic_quench(t, total_steps): return 1 / (1 + 0.0001 * t**2)

# Epsilon-Greedy Algorithm
def epsilon_greedy_quench(quench_func, steps=10000):
    action_values = np.zeros(20)  # Estimated action values
    action_counts = np.zeros(20)  # How often each action has been selected
    rewards = []  # To store the rewards at each step
    true_rewards = get_probabilities()  # True reward distribution

    for step in range(steps):
        if np.random.random() < quench_func(step, steps):  # Exploration
            action = np.random.choice(20)
        else:  # Exploitation
            action = np.argmax(action_values)
        
        # Get the reward for the chosen action
        reward = np.random.normal(true_rewards[action], 1)
        rewards.append(reward)

        # Update the action-value estimate using incremental formula
        action_counts[action] += 1
        action_values[action] += (reward - action_values[action]) / action_counts[action]

    return rewards

# Epsilon-Greedy Algorithm
def epsilon_greedy(epsilon, steps=10000):
    action_values = np.zeros(20)  # Estimated action values
    action_counts = np.zeros(20)  # How often each action has been selected
    rewards = []  # To store the rewards at each step
    true_rewards = get_probabilities()  # True reward distribution

    for step in range(steps):
        if np.random.random() < epsilon:  # Exploration
            action = np.random.choice(20)
        else:  # Exploitation
            action = np.argmax(action_values)
        
        # Get the reward for the chosen action
        reward = np.random.normal(true_rewards[action], 1)
        rewards.append(reward)

        # Update the action-value estimate using incremental formula
        action_counts[action] += 1
        action_values[action] += (reward - action_values[action]) / action_counts[action]

    return rewards


# Plot both algorithms on the same graph
def plot_rewards(rewards_list, labels):
    for rewards, label in zip(rewards_list, labels):
        plt.plot(np.cumsum(rewards) / (np.arange(len(rewards)) + 1), label=label)
    plt.xlabel('Steps')
    plt.ylabel('Average Reward')
    plt.legend()
    plt.title('Epsilon-Greedy with Quenching')
    plt.show()

# Main logic to run both algorithms
epsilon_funcs = [linear_quench, asymptotic_quench, heavy_asymptotic_quench]
rewards_list = [epsilon_greedy_quench(epsilon_func) for epsilon_func in epsilon_funcs]
labels = ["Linear Quench", "Asymptotic Quench", "Heavy Asymptotic Quench"]

# Plot all together
plot_rewards(rewards_list, labels)
