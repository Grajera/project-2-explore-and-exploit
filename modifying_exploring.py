import numpy as np
import matplotlib.pyplot as plt

# Function to generate reward probabilities
def get_probabilities():
    probs = [
        np.random.normal(0, 5),
        np.random.normal(-0.5, 12),
        np.random.normal(2, 3.9),
        np.random.normal(-0.5, 7),
        np.random.normal(-1.2, 8),
        np.random.normal(-3, 7),
        np.random.normal(-10, 20),
        np.random.normal(-0.5, 1),
        np.random.normal(-1, 2),
        np.random.normal(1, 6),
        np.random.normal(0.7, 4),
        np.random.normal(-6, 11),
        np.random.normal(-7, 1),
        np.random.normal(-0.5, 2),
        np.random.normal(-6.5, 1),
        np.random.normal(-3, 6),
        np.random.normal(0, 8),
        np.random.normal(2, 3.9),
        np.random.normal(-9, 12),
        np.random.normal(-1, 6),
        np.random.normal(-4.5, 8)
    ]
    return probs


# Avoids current 'best' choice to explore more broadly
def epsilon_greedy_exclude_best(epsilon, steps=10000):
    action_values = np.zeros(20)  # Estimated action values
    action_counts = np.zeros(20)  # How often each action has been selected
    rewards = []  # To store the rewards at each step
    true_rewards = get_probabilities()  # True reward distribution

    for step in range(steps):
        # Avoids best choice
        current_best = np.argmax(action_values)

        if np.random.random() < epsilon:
            action = np.random.choice([i for i in range(20) if i != current_best])
        else:
            action = current_best

        # Get the reward for the chosen action
        reward = np.random.normal(true_rewards[action], 1)
        rewards.append(reward)

        # Update the action-value estimate using incremental formula
        action_counts[action] += 1
        action_values[action] += (reward - action_values[action]) / action_counts[action]

    return rewards

# Prioritizes options that have been chosen fewer times.
def weighted_exploration(epsilon, steps=10000):
    action_values = np.zeros(20)  # Estimated action values
    action_counts = np.zeros(20)  # How often each action has been selected
    rewards = []  # To store the rewards at each step
    true_rewards = get_probabilities()  # True reward distribution

    for step in range(steps):
        if np.random.random() < epsilon:
            unexplored_probs = 1 / (action_counts + 1)  # Higher probability for less explored choices
            action = np.random.choice(20, p=unexplored_probs / unexplored_probs.sum())
        else:
            action = np.argmax(action_values)

        # Get the reward for the chosen action
        reward = np.random.normal(true_rewards[action], 1)
        rewards.append(reward)

        # Update the action-value estimate using incremental formula
        action_counts[action] += 1
        action_values[action] += (reward - action_values[action]) / action_counts[action]

    return rewards

# Epsilon-Greedy Algorithm
def epsilon_greedy(epsilon, steps=10000):
    action_values = np.zeros(20)  # Estimated action values
    action_counts = np.zeros(20)  # How often each action has been selected
    rewards = []  # To store the rewards at each step
    true_rewards = get_probabilities()  # True reward distribution

    for step in range(steps):
        if np.random.random() < epsilon:  # Exploration
            action = np.random.choice(20)
        else:  # Exploitation
            action = np.argmax(action_values)
        
        # Get the reward for the chosen action
        reward = np.random.normal(true_rewards[action], 1)
        rewards.append(reward)

        # Update the action-value estimate using incremental formula
        action_counts[action] += 1
        action_values[action] += (reward - action_values[action]) / action_counts[action]

    return rewards

# Plot both algorithms on the same graph
def plot_rewards(rewards_list, labels):
    for rewards, label in zip(rewards_list, labels):
        plt.plot(np.cumsum(rewards) / (np.arange(len(rewards)) + 1), label=label)
    plt.xlabel('Steps')
    plt.ylabel('Average Reward')
    plt.legend()
    plt.title('Modifying Exploring')
    plt.show()

# Main logic to run all the algorithms
epsilon = 0.1
rewards_list = [epsilon_greedy(epsilon), weighted_exploration(epsilon), epsilon_greedy_exclude_best(epsilon)]
labels = ["Epsilon-Greedy", "Weighted Exploration", "Exclude Best"]



# Plot all together
plot_rewards(rewards_list, labels)
