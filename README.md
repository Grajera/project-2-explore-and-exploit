# Explore-and-Exploit

## Team Members
Michael Grajera

Benson Shafer

## Required Packages

Install the required Python libraries using `pip`. Run the following commands to install all dependencies:

* matplotlib
```bash
pip install matplotlib
```
* numpy
```bash
pip install numpy
```

## Running the File
For Part 1: Epsilon-Greedy Algorithm in a Static Environment
```bash
Run the file Epsilon-Greedy Alforithm.py
```
For Part 2: Exploring Epsilon in the Epsilon-Greedy Algorithm (Task 1)
```bash
Run the file exploring_espilon.py
```
For Part 2: Exploring Epsilon in the Epsilon-Greedy Algorithm (Task 2)
```bash
Run the file modifying_exploring.py
```
For Part 3: Moving Bandits – Simulating Market Dynamics
```bash
Run the file Dynamic Epsilon-Greedy Algorithm.py
```

# Part 1: Epsilon-Greedy Algorithm in a Static Environment
### 1. Epsilon-Greedy Algorithm

a. Different Epsilon Values

The values of epsilon tested are ϵ=0.01, 0.05, 0.1, 0.4. 

These represent different levels of exploration (randomly choosing an action):

    ϵ=0.01: Minimal exploration, mostly exploiting the best-known action.
    ϵ=0.05: Somewhat more exploration than 0.01, but still primarily exploiting.
    ϵ=0.1: Balanced exploration and exploitation, exploring often while exploiting the best-known action most of the time.
    ϵ=0.4: Much higher exploration, frequently testing new actions.

b. Convergence Rates for Each Epsilon Value

The exploration-exploitation trade-off refers to balancing:
    
    NOTE: For the following results see the graph below.

    ϵ=0.01: Converged quickly to a suboptimal action. Minimal exploration limited the ability to find better actions.
    
    ϵ=0.05: Converged slower than 0.01 but discovered better actions over time due to slightly more exploration.
    
    ϵ=0.1: Showed the best overall performance, balancing exploration and exploitation to converge to a high average reward.
    
    ϵ=0.4: Explored too much, leading to slower convergence and lower average reward compared to 0.05 and 0.1..

For lower epsilon values like 0.01, the algorithm will focus on exploitation, which can lead to faster convergence but potentially miss better actions. For higher values like 0.4, it will explore more, leading to slower convergence but a higher chance of finding the best action.

C. Optimal Epsilon Value

    Based on the results, ϵ=0.1 gave the best performance, achieving a good balance between exploration and exploitation. It converged faster and to a higher average reward compared to other epsilon values.

### 2. Thompson Sampling Algorithm

In comparison to the epsilon-greedy method, the Thompson Sampling method uses a probabilistic approach to select actions based on their uncertainty. It adjusts the likelihood of selecting an action using Bayesian updating (successes and failures).

1. Convergence Speed

    <b>Thompson Sampling:</b>

        Faster Convergence: Thompson Sampling converges more quickly because it uses Bayesian updating to dynamically adjust exploration based on the uncertainty of each action's reward. This allows it to focus on the most promising actions early on, leading to quicker identification of the best action.

        By sampling from a probability distribution (Beta distribution), it explores when uncertain and exploits when confident, minimizing unnecessary exploration.

    <b>Epsilon-Greedy (ε = 0.1):</b>

        Slower Convergence: Epsilon-greedy with ε = 0.1 converges slower compared to Thompson Sampling. With a fixed 10% exploration rate, the algorithm continues to explore even after it has identified the best action, which slows down its convergence.

        The fixed exploration-exploitation balance does not adapt to new information about the actions, making it less efficient in honing in on the optimal solution.

2. Performance (Cumulative Reward)

    <b>Thompson Sampling:</b>

        Higher Performance: Thompson Sampling tends to yield higher cumulative rewards over time. This is because it rapidly reduces exploration once a good action is identified and focuses on exploiting the best actions. As a result, it accumulates more rewards faster than epsilon-greedy.
        
        It also adjusts more intelligently to varying reward distributions, which allows it to achieve better performance in more complex environments.

    <b>Epsilon-Greedy (ε = 0.1):</b>
    
        Moderate Performance: While epsilon-greedy with ε = 0.1 performs reasonably well, it suffers from unnecessary exploration. The algorithm still chooses random actions even when the best option has already been discovered. This leads to lower total rewards compared to Thompson Sampling.
    
        The fixed exploration means that even if the environment becomes well understood, the algorithm does not shift its strategy to exploit known good actions more aggressively.

3. Adaptation to Uncertainty

    <b>Thompson Sampling:</b>
    
        Better Adaptation: One of Thompson Sampling's key strengths is its ability to adapt to uncertainty. It balances exploration based on how confident it is in its estimates of each action’s reward. When it is highly uncertain, it explores more. As the uncertainty decreases, it exploits more, allowing for more informed decision-making.
    
    <b>Epsilon-Greedy (ε = 0.1):</b>

        Less Adaptation: Epsilon-greedy with a fixed ε does not adapt to uncertainty. Even when the algorithm is confident about an action, it still continues to explore, which may lead to inefficient use of steps and lower overall rewards.

Summary of Results:

    Thompson Sampling: This method adapted more dynamically, adjusting exploration based on uncertainty about each action. As a result, it consistently outperformed epsilon-greedy in both convergence speed and total reward accumulated over time.

    Epsilon-Greedy (0.1): Although it performed well, it was less adaptive and relied on a fixed exploration-exploitation strategy. Thompson Sampling was able to adjust more effectively and thus converge faster to the optimal solution.

## Results Graph:
![image](Images/EpsilonVsThompson.png)


# Part 2: Exploring Epsilon in the Epsilon-Greedy Algorithm
### 1. Epsilon Quenching Functions

a. Different Quenching Functions

There are three different Quenching Functions being tested, linear quenching, asymptotic quenching, and heavily asymptotic quenching.

Each function gradually reduced epsilon over time allowing the algorithm to prioritize exploring early in the test and then transition
into more exploitation later. The three functions each had a different way to quench epsilon:

    Linear Quenching: Epsilon starts at 1 and decreases linearly to 0.
    Asymptotic Quenching: Epsilon starts at 1 and approaches 0 asymptotically.
    Heavily Asymptotic Quenching: Epsilon starts at 1 and stays there for many steps, only approaching 0 near the end of the steps.

b. Convergence Rates for Each Quenching Strategy

    Linear Quenching: Increases reward at a linear pace, converging very slowly.
    
    The other two similarly convergance at a balanced pace and the better strategy shifts between them currently.
    Because they both jump around on which one gives the higher reward it is hard to decide which is more optimal for converging.
    From just a face value of multiple tests Asymptotic on Average is higher but more thorough testing would be needed to prove that.

c. Optimal Quenching Function for Balance Between Exploration and Exploitation

    The most optimal quenching function for balance though seems to be asymptotic due to the slow quench. Heavily asymptotic, while similar
    spends a long time leaving it open for exploring which isn't as balanced. That being said it's accuracy is also often on par with the others.

## Results Graph:
![image](Images/Quenching.png)

### 2. Modifying Exploration Strategy

a. Different Exploration Strategies

The two new strategies are 'exploring away from the current best choice' and 'higher exploration of unexplored regions'.
These are both being compared against the original epsilon greedy algorithm.

Exploring away from the current boest choice is used to avoid what is currently the 'best' in hopes of finding something better.
Due to this it will not continually explore something that doesn't need to be explored right now (i.e. the 'best' current option).

The strategy behind a higher exploration of unexplored regions is to cover more choices. This should help find the best solution
but might struggle by skipping quickly over 'good' solutions.

b. Convergence Speed and Accuracy for each strategy and reflection.

    The strategies all tend to converge at simlar speed but there have been rare cases for epsilon-greedy to take significantly longer than the
    other two.

    On average exluding the best choice seemed to preform the best but the strategies fluctuated widly between which one performed better. It
    was very difficult without more testing to determine which would perform the best.

## Results Graph:
![image](Images/ExploringStrategies.png)

# Part 3: Moving Bandits – Simulating Market Dynamics.

Thompson Sampling vs. Epsilon-Greedy:

### Comparing Their Performance in This Dynamic Environment
In the dynamic environment where bandit rewards drift over time and undergo sudden shifts at step 3,000, Thompson sampling generally performs better than epsilon-greedy. This is due to its inherent adaptability, which allows it to update beliefs about each bandit based on past rewards and continue to explore optimally even when the environment changes.

Thompson Sampling adapts more quickly to the sudden shifts in bandit means, especially when a drastic shift (like Bandit 7’s large potential rewards of 50 when the reward is more than 3 standard deviations away) occurs. Its posterior distribution naturally accounts for the dynamic changes, making it more flexible compared to epsilon-greedy.

Epsilon-Greedy, while able to explore new options, tends to suffer from the need to balance exploration and exploitation. In the dynamic environment, when bandits experience sudden shifts, the epsilon-greedy algorithm may still be stuck exploiting previous best actions for too long, thus slowing down the discovery of newly favorable bandits after the shift.

### Restarting Thompson Sampling at Step 3,000

Restarting Thompson sampling at step 3,000 leads to faster convergence compared to letting the algorithm continue without a reset. This is particularly useful when bandits undergo large shifts in their reward distributions (e.g., Bandit 0 shifting by +7).

Without a restart, Thompson sampling relies on prior knowledge, which could delay convergence when large changes occur, as it takes longer to adjust. However, after a restart, the algorithm explores the new environment from scratch and quickly identifies the shifted bandits, speeding up convergence.

By resetting at step 3,000, Thompson sampling can adapt to the new reward landscape faster, especially in cases of significant changes like those in Bandit 7 and Bandit 0.

## 1: What role does randomness play in helping or hindering convergence under dynamic conditions?

Randomness plays a significant role in both exploration and exploitation. In dynamic conditions, randomness helps algorithms like epsilon-greedy to explore new options when there are shifts in the environment, such as the sudden changes at step 3,000. This ensures that the algorithm does not get stuck exploiting a suboptimal action when better alternatives emerge after sudden shifts.

Helping:

 Randomness helps in the exploration phase by allowing the algorithm to test different bandits. In the epsilon-greedy algorithm, randomness in action selection (based on epsilon) ensures that even when the algorithm has settled on a bandit, it still explores other options, which is crucial when bandits change their behavior over time.
        
In Thompson sampling, the randomness in sampling from the posterior distributions allows the algorithm to adapt to changes dynamically, favoring exploration when there's uncertainty and exploitation when a bandit shows consistently better performance.

Hindering:

However, randomness can also slow down convergence, especially if the exploration rate is too high. For example, in the epsilon-greedy algorithm, a high epsilon value may cause the algorithm to explore too frequently, even when the environment is relatively stable. This may delay convergence, especially if the best bandit has already been identified.

In general, randomness is essential for adaptability, but excessive randomness can hinder efficient convergence. Balancing exploration and exploitation is key.

## 2: How does the modified epsilon-greedy algorithm compare to the standard approach, particularly in terms of adaptability?

The modified epsilon-greedy algorithm with drifting bandits and sudden shifts introduces more complexity compared to a static environment. Here's how it compares:

Standard Epsilon-Greedy:

In a static environment, once the optimal bandit is identified, the algorithm can simply exploit that choice more frequently as epsilon decreases.

It converges faster if the environment remains stable since there’s little need for exploration after the best option is known.

Modified Epsilon-Greedy (Dynamic Environment):

In the dynamic environment, bandit rewards are drifting over time, and some bandits undergo sudden shifts (e.g., Bandit 7's special rule). The modified epsilon-greedy algorithm needs to continuously explore to adapt to these changes.

The drifting mean of each bandit makes it harder for the algorithm to stay focused on one action. The sudden shifts at step 3,000 further force the algorithm to reconsider previously suboptimal actions.

Adaptability: The modified epsilon-greedy algorithm is more adaptive than the standard approach because the added exploration helps adjust to dynamic changes. However, it may suffer from slower convergence because of the need to explore more frequently.

In summary, while the modified epsilon-greedy algorithm is more adaptive in a dynamic environment, its convergence speed can be slower compared to the standard version in static environments.

## 3: Does restarting Thompson sampling at step 3,000 lead to faster convergence compared to letting the algorithm continue?

Restarting Thompson sampling at step 3,000 does have implications for convergence:

Thompson Sampling Without Restart:
       
Without restarting, the algorithm continues with the prior knowledge accumulated before the sudden shift. This could be advantageous if the shifts are small, as the algorithm might still make use of past knowledge. However, in cases where the shift is large (e.g., Bandit 0 with a +7 shift), the algorithm might need time to adjust its belief, leading to slower convergence.

Thompson Sampling With Restart:

Restarting at step 3,000 resets the algorithm’s beliefs about the bandits. This can lead to faster convergence after the sudden shift, especially when some bandits undergo large reward changes. By resetting, the algorithm effectively "forgets" the previous environment, which may no longer be relevant, and begins fresh exploration based on the new conditions.
        
For example, Bandit 7's rule (with potential rewards of 50) may be more easily identified if the algorithm restarts, as it will explore it sooner rather than relying on outdated prior data.

In this case, restarting Thompson sampling can lead to faster convergence in environments with significant shifts, as it helps the algorithm more quickly adapt to the new reward distribution.

## Results Graph:
![Alt text](<Images/Part 3 Graph.png>)