## Part 2: Exploring Epsilon in the Epsilon-Greedy Algorithm
### 1. Epsilon Quenching Functions

a. Different Quenching Functions

There are three different Quenching Functions being tested, linear quenching, asymptotic quenching, and heavily asymptotic quenching.

Each function gradually reduced epsilon over time allowing the algorithm to prioritize exploring early in the test and then transition
into more exploitation later. The three functions each had a different way to quench epsilon:

    Linear Quenching: Epsilon starts at 1 and decreases linearly to 0.
    Asymptotic Quenching: Epsilon starts at 1 and approaches 0 asymptotically.
    Heavily Asymptotic Quenching: Epsilon starts at 1 and stays there for many steps, only approaching 0 near the end of the steps.

b. Convergence Rates for Each Quenching Strategy

    Linear Quenching: Increases reward at a linear pace, converging very slowly.
    
    The other two similarly convergance at a balanced pace and the better strategy shifts between them currently.
    Because they both jump around on which one gives the higher reward it is hard to decide which is more optimal for converging.
    From just a face value of multiple tests Asymptotic on Average is higher but more thorough testing would be needed to prove that.

c. Optimal Quenching Function for Balance Between Exploration and Exploitation

    The most optimal quenching function for balance though seems to be asymptotic due to the slow quench. Heavily asymptotic, while similar
    spends a long time leaving it open for exploring which isn't as balanced. That being said it's accuracy is also often on par with the others.

## Results Graph:
![image](Images/Quenching.png)

### 2. Modifying Exploration Strategy

a. Different Exploration Strategies

The two new strategies are 'exploring away from the current best choice' and 'higher exploration of unexplored regions'.
These are both being compared against the original epsilon greedy algorithm.

Exploring away from the current boest choice is used to avoid what is currently the 'best' in hopes of finding something better.
Due to this it will not continually explore something that doesn't need to be explored right now (i.e. the 'best' current option).

The strategy behind a higher exploration of unexplored regions is to cover more choices. This should help find the best solution
but might struggle by skipping quickly over 'good' solutions.

b. Convergence Speed and Accuracy for each strategy and reflection.

    The strategies all tend to converge at simlar speed but there have been rare cases for epsilon-greedy to take significantly longer than the
    other two.

    On average exluding the best choice seemed to preform the best but the strategies fluctuated widly between which one performed better. It
    was very difficult without more testing to determine which would perform the best.

## Results Graph:
![image](Images/ExploringStrategies.png)
