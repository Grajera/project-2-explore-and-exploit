import numpy as np
import matplotlib.pyplot as plt

# Function to generate reward probabilities
def get_probabilities():
    probs = [
        np.random.normal(0, 5),
        np.random.normal(-0.5, 12),
        np.random.normal(2, 3.9),
        np.random.normal(-0.5, 7),
        np.random.normal(-1.2, 8),
        np.random.normal(-3, 7),
        np.random.normal(-10, 20),
        np.random.normal(-0.5, 1),
        np.random.normal(-1, 2),
        np.random.normal(1, 6),
        np.random.normal(0.7, 4),
        np.random.normal(-6, 11),
        np.random.normal(-7, 1),
        np.random.normal(-0.5, 2),
        np.random.normal(-6.5, 1),
        np.random.normal(-3, 6),
        np.random.normal(0, 8),
        np.random.normal(2, 3.9),
        np.random.normal(-9, 12),
        np.random.normal(-1, 6),
        np.random.normal(-4.5, 8)
    ]
    return probs

# Epsilon-Greedy Algorithm
def epsilon_greedy(epsilon, steps=10000):
    action_values = np.zeros(20)  # Estimated action values
    action_counts = np.zeros(20)  # How often each action has been selected
    rewards = []  # To store the rewards at each step
    true_rewards = get_probabilities()  # True reward distribution

    for step in range(steps):
        if np.random.random() < epsilon:  # Exploration
            action = np.random.choice(20)
        else:  # Exploitation
            action = np.argmax(action_values)
        
        # Get the reward for the chosen action
        reward = np.random.normal(true_rewards[action], 1)
        rewards.append(reward)

        # Update the action-value estimate using incremental formula
        action_counts[action] += 1
        action_values[action] += (reward - action_values[action]) / action_counts[action]

    return rewards

# Thompson Sampling Algorithm
def thompson_sampling(steps=10000):
    action_values = np.zeros(20)
    action_counts = np.zeros(20)
    true_rewards = get_probabilities()

    alpha = np.ones(20)  # Successes
    beta = np.ones(20)  # Failures
    rewards = []

    for step in range(steps):
        sampled_probs = np.random.beta(alpha, beta)
        action = np.argmax(sampled_probs)

        reward = np.random.normal(true_rewards[action], 1)
        rewards.append(reward)

        # Update beta distribution
        action_counts[action] += 1
        if reward > 0:
            alpha[action] += 1
        else:
            beta[action] += 1

    return rewards

# Plot both algorithms on the same graph
def plot_rewards(rewards_list, labels):
    for rewards, label in zip(rewards_list, labels):
        plt.plot(np.cumsum(rewards) / (np.arange(len(rewards)) + 1), label=label)
    plt.xlabel('Steps')
    plt.ylabel('Average Reward')
    plt.legend()
    plt.title('Epsilon-Greedy vs Thompson Sampling')
    plt.show()

# Main logic to run both algorithms
epsilons = [0.01, 0.05, 0.1, 0.4]
rewards_list = [epsilon_greedy(epsilon) for epsilon in epsilons]
thompson_rewards = thompson_sampling()

# Add Thompson Sampling to the list
rewards_list.append(thompson_rewards)
labels = [f"Epsilon={epsilon}" for epsilon in epsilons] + ["Thompson Sampling"]

# Plot all together
plot_rewards(rewards_list, labels)
