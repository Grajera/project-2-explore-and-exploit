1. Epsilon-Greedy Algorithm

a. Different Epsilon Values

The values of epsilon tested are ϵ=0.01, 0.05, 0.1, 0.4. 

These represent different levels of exploration (randomly choosing an action):

    ϵ=0.01: Minimal exploration, mostly exploiting the best-known action.
    ϵ=0.05: Somewhat more exploration than 0.01, but still primarily exploiting.
    ϵ=0.1: Balanced exploration and exploitation, exploring often while exploiting the best-known action most of the time.
    ϵ=0.4: Much higher exploration, frequently testing new actions.

b. Convergence Rates for Each Epsilon Value

The exploration-exploitation trade-off refers to balancing:

    NOTE: For the following results see the graph below.

    ϵ=0.01: Converged quickly to a suboptimal action. Minimal exploration limited the ability to find better actions.
    
    ϵ=0.05: Converged slower than 0.01 but discovered better actions over time due to slightly more exploration.
    
    ϵ=0.1: Showed the best overall performance, balancing exploration and exploitation to converge to a high average reward.
    
    ϵ=0.4: Explored too much, leading to slower convergence and lower average reward compared to 0.05 and 0.1..

For lower epsilon values like 0.01, the algorithm will focus on exploitation, which can lead to faster convergence but potentially miss better actions. For higher values like 0.4, it will explore more, leading to slower convergence but a higher chance of finding the best action.

c. Optimal Epsilon Value

Based on the results, ϵ=0.1 gave the best performance, achieving a good balance between exploration and exploitation. It converged faster and to a higher average reward compared to other epsilon values.

2. Thompson Sampling Algorithm

In comparison to the epsilon-greedy method, the Thompson Sampling method uses a probabilistic approach to select actions based on their uncertainty. It adjusts the likelihood of selecting an action using Bayesian updating (successes and failures).

1. Convergence Speed

    <b>Thompson Sampling:</b>

    Faster Convergence: Thompson Sampling converges more quickly because it uses Bayesian updating to dynamically adjust exploration based on the uncertainty of each action's reward. This allows it to focus on the most promising actions early on, leading to quicker identification of the best action.

    By sampling from a probability distribution (Beta distribution), it explores when uncertain and exploits when confident, minimizing unnecessary exploration.

    <b>Epsilon-Greedy (ε = 0.1):</b>

    Slower Convergence: Epsilon-greedy with ε = 0.1 converges slower compared to Thompson Sampling. With a fixed 10% exploration rate, the algorithm continues to explore even after it has identified the best action, which slows down its convergence.

    The fixed exploration-exploitation balance does not adapt to new information about the actions, making it less efficient in honing in on the optimal solution.

2. Performance (Cumulative Reward)

    <b>Thompson Sampling:</b>
    Higher Performance: Thompson Sampling tends to yield higher cumulative rewards over time. This is because it rapidly reduces exploration once a good action is identified and focuses on exploiting the best actions. As a result, it accumulates more rewards faster than epsilon-greedy.
    It also adjusts more intelligently to varying reward distributions, which allows it to achieve better performance in more complex environments.

    <b>Epsilon-Greedy (ε = 0.1):</b>
    
    Moderate Performance: While epsilon-greedy with ε = 0.1 performs reasonably well, it suffers from unnecessary exploration. The algorithm still chooses random actions even when the best option has already been discovered. This leads to lower total rewards compared to Thompson Sampling.
    
    The fixed exploration means that even if the environment becomes well understood, the algorithm does not shift its strategy to exploit known good actions more aggressively.

3. Adaptation to Uncertainty

    <b>Thompson Sampling:</b>
    
    Better Adaptation: One of Thompson Sampling's key strengths is its ability to adapt to uncertainty. It balances exploration based on how confident it is in its estimates of each action’s reward. When it is highly uncertain, it explores more. As the uncertainty decreases, it exploits more, allowing for more informed decision-making.
    
    <b>Epsilon-Greedy (ε = 0.1):</b>
    
    Less Adaptation: Epsilon-greedy with a fixed ε does not adapt to uncertainty. Even when the algorithm is confident about an action, it still continues to explore, which may lead to inefficient use of steps and lower overall rewards.

Summary of Results:

    Thompson Sampling: This method adapted more dynamically, adjusting exploration based on uncertainty about each action. As a result, it consistently outperformed epsilon-greedy in both convergence speed and total reward accumulated over time.

    Epsilon-Greedy (0.1): Although it performed well, it was less adaptive and relied on a fixed exploration-exploitation strategy. Thompson Sampling was able to adjust more effectively and thus converge faster to the optimal solution.

![image](Images/EpsilonVsThompson.png)

