## Thompson vs Greedy Algorithm Notes and Observations.

Thompson Sampling vs. Epsilon-Greedy:

### Comparing Their Performance in This Dynamic Environment
In the dynamic environment where bandit rewards drift over time and undergo sudden shifts at step 3,000, Thompson sampling generally performs better than epsilon-greedy. This is due to its inherent adaptability, which allows it to update beliefs about each bandit based on past rewards and continue to explore optimally even when the environment changes.

Thompson Sampling adapts more quickly to the sudden shifts in bandit means, especially when a drastic shift (like Bandit 7’s large potential rewards of 50 when the reward is more than 3 standard deviations away) occurs. Its posterior distribution naturally accounts for the dynamic changes, making it more flexible compared to epsilon-greedy.

Epsilon-Greedy, while able to explore new options, tends to suffer from the need to balance exploration and exploitation. In the dynamic environment, when bandits experience sudden shifts, the epsilon-greedy algorithm may still be stuck exploiting previous best actions for too long, thus slowing down the discovery of newly favorable bandits after the shift.

### Restarting Thompson Sampling at Step 3,000

Restarting Thompson sampling at step 3,000 leads to faster convergence compared to letting the algorithm continue without a reset. This is particularly useful when bandits undergo large shifts in their reward distributions (e.g., Bandit 0 shifting by +7).

Without a restart, Thompson sampling relies on prior knowledge, which could delay convergence when large changes occur, as it takes longer to adjust. However, after a restart, the algorithm explores the new environment from scratch and quickly identifies the shifted bandits, speeding up convergence.

By resetting at step 3,000, Thompson sampling can adapt to the new reward landscape faster, especially in cases of significant changes like those in Bandit 7 and Bandit 0.

## 1: What role does randomness play in helping or hindering convergence under dynamic conditions?

Randomness plays a significant role in both exploration and exploitation. In dynamic conditions, randomness helps algorithms like epsilon-greedy to explore new options when there are shifts in the environment, such as the sudden changes at step 3,000. This ensures that the algorithm does not get stuck exploiting a suboptimal action when better alternatives emerge after sudden shifts.

Helping:

 Randomness helps in the exploration phase by allowing the algorithm to test different bandits. In the epsilon-greedy algorithm, randomness in action selection (based on epsilon) ensures that even when the algorithm has settled on a bandit, it still explores other options, which is crucial when bandits change their behavior over time.
        
In Thompson sampling, the randomness in sampling from the posterior distributions allows the algorithm to adapt to changes dynamically, favoring exploration when there's uncertainty and exploitation when a bandit shows consistently better performance.

Hindering:

However, randomness can also slow down convergence, especially if the exploration rate is too high. For example, in the epsilon-greedy algorithm, a high epsilon value may cause the algorithm to explore too frequently, even when the environment is relatively stable. This may delay convergence, especially if the best bandit has already been identified.

In general, randomness is essential for adaptability, but excessive randomness can hinder efficient convergence. Balancing exploration and exploitation is key.

## 2: How does the modified epsilon-greedy algorithm compare to the standard approach, particularly in terms of adaptability?

The modified epsilon-greedy algorithm with drifting bandits and sudden shifts introduces more complexity compared to a static environment. Here's how it compares:

Standard Epsilon-Greedy:

In a static environment, once the optimal bandit is identified, the algorithm can simply exploit that choice more frequently as epsilon decreases.

It converges faster if the environment remains stable since there’s little need for exploration after the best option is known.

Modified Epsilon-Greedy (Dynamic Environment):

In the dynamic environment, bandit rewards are drifting over time, and some bandits undergo sudden shifts (e.g., Bandit 7's special rule). The modified epsilon-greedy algorithm needs to continuously explore to adapt to these changes.

The drifting mean of each bandit makes it harder for the algorithm to stay focused on one action. The sudden shifts at step 3,000 further force the algorithm to reconsider previously suboptimal actions.

Adaptability: The modified epsilon-greedy algorithm is more adaptive than the standard approach because the added exploration helps adjust to dynamic changes. However, it may suffer from slower convergence because of the need to explore more frequently.

In summary, while the modified epsilon-greedy algorithm is more adaptive in a dynamic environment, its convergence speed can be slower compared to the standard version in static environments.

## 3: Does restarting Thompson sampling at step 3,000 lead to faster convergence compared to letting the algorithm continue?

Restarting Thompson sampling at step 3,000 does have implications for convergence:

Thompson Sampling Without Restart:
       
Without restarting, the algorithm continues with the prior knowledge accumulated before the sudden shift. This could be advantageous if the shifts are small, as the algorithm might still make use of past knowledge. However, in cases where the shift is large (e.g., Bandit 0 with a +7 shift), the algorithm might need time to adjust its belief, leading to slower convergence.

Thompson Sampling With Restart:

Restarting at step 3,000 resets the algorithm’s beliefs about the bandits. This can lead to faster convergence after the sudden shift, especially when some bandits undergo large reward changes. By resetting, the algorithm effectively "forgets" the previous environment, which may no longer be relevant, and begins fresh exploration based on the new conditions.
        
For example, Bandit 7's rule (with potential rewards of 50) may be more easily identified if the algorithm restarts, as it will explore it sooner rather than relying on outdated prior data.

In this case, restarting Thompson sampling can lead to faster convergence in environments with significant shifts, as it helps the algorithm more quickly adapt to the new reward distribution.

## Results Graph:
![Alt text](images/Part%203%20Graph.png)